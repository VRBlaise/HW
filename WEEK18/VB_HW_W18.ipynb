{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20dbcac",
   "metadata": {},
   "source": [
    "### 1.\tWhat is a neural network? What are the general steps required to build a neural network? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b599c0b1",
   "metadata": {},
   "source": [
    "Neural networks are collections of connected nodes (which represent neurons), that aim to mimic the workings of neurons in the brain. The nodes are connected by edges (loosely analogous to dendrites) which represent (directional) transmission of information between the nodes. The inputs to a node a subject to some nonlinear transformation in order to create the output, which is sent forward if it reaches a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5e409",
   "metadata": {},
   "source": [
    "Neural networks are aggregated into layers which we need to build:\n",
    "\n",
    "\n",
    "    - initialize the model\n",
    "    - we add input layer, hidden layers, output layer (we need to specify how many hidden layers we want and also how many nodes for each layer: the number of nodes in the hidden layers is subject to optimalization and the output would be as many as the number of possible classes in a classification problem and a single node for regression)\n",
    "    - choose an activation function for the hidden layers (relu, sigmoid, tanh), this would depend on the type of network used\n",
    "    - choose an activation function for the output layer that is most appropriate for the type of problem (e.g.: binary-sigmoid, nonbinary classification - softmax, linear regression - linear activation)\n",
    "    - the inputs to these functions are usually weighted, so we need to initialize the weights (and bias which is constant), as far as I know this is done for us in keras, by using random weights to begin with.\n",
    "    - we need to define (or choose a predefined) a loss function that is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b300e",
   "metadata": {},
   "source": [
    "### 2.\tGenerally, how do you check the performance of a neural network? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbaa3e2",
   "metadata": {},
   "source": [
    "The performance metrics (besides speed) would depend on the problem to be solved:\n",
    "\n",
    "- for a regression problem the mean squared error (MSE) or its square route are a good metric\n",
    "- for a classification problem accuracy, recall (if true positives are the most important class) or precision, and if we want to check the robustness of the system, then ROC-AUC might be the right metric\n",
    "\n",
    "Now, this implies the performance metrics shall depend on the problem, rather than the method used, this way we can directly compare different methods like KNN, DecisionTree or neural networks. If the performance in the chosen metrics is close across models, then speed, simplicity of implementation and scalability should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14e603",
   "metadata": {},
   "source": [
    "### 3.\tCreate a neural network using keras to predict the outcome of either of the arrythmia or abalone datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f59737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610384e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title of Database: Abalone data\n",
      "\n",
      "2. Sources:\n",
      "\n",
      "   (a) Original owners of database:\n",
      "\tMarine Resources Division\n",
      "\tMarine Research Laboratories - Taroona\n",
      "\tDepartment of Primary Industry and Fisheries, Tasmania\n",
      "\tGPO Box 619F, Hobart, Tasmania 7001, Australia\n",
      "\t(contact: Warwick Nash +61 02 277277, wnash@dpi.tas.gov.au)\n",
      "\n",
      "   (b) Donor of database:\n",
      "\tSam Waugh (Sam.Waugh@cs.utas.edu.au)\n",
      "\tDepartment of Computer Science, University of Tasmania\n",
      "\tGPO Box 252C, Hobart, Tasmania 7001, Australia\n",
      "\n",
      "   (c) Date received: December 1995\n",
      "\n",
      "\n",
      "3. Past Usage:\n",
      "\n",
      "   Sam Waugh (1995) \"Extending and benchmarking Cascade-Correlation\", PhD\n",
      "   thesis, Computer Science Department, University of Tasmania.\n",
      "\n",
      "   -- Test set performance (final 1044 examples, first 3133 used for training):\n",
      "\t24.86% Cascade-Correlation (no hidden nodes)\n",
      "\t26.25% Cascade-Correlation (5 hidden nodes)\n",
      "\t21.5%  C4.5\n",
      "\t 0.0%  Linear Discriminate Analysis\n",
      "\t 3.57% k=5 Nearest Neighbour\n",
      "      (Problem encoded as a classification task)\n",
      "\n",
      "   -- Data set samples are highly overlapped.  Further information is required\n",
      "\tto separate completely using affine combinations.  Other restrictions\n",
      "\tto data set examined.\n",
      "\n",
      "   David Clark, Zoltan Schreter, Anthony Adams \"A Quantitative Comparison of\n",
      "   Dystal and Backpropagation\", submitted to the Australian Conference on\n",
      "   Neural Networks (ACNN'96). Data set treated as a 3-category classification\n",
      "   problem (grouping ring classes 1-8, 9 and 10, and 11 on).\n",
      "\n",
      "   -- Test set performance (3133 training, 1044 testing as above):\n",
      "\t64%    Backprop\n",
      "\t55%    Dystal\n",
      "   -- Previous work (Waugh, 1995) on same data set:\n",
      "\t61.40% Cascade-Correlation (no hidden nodes)\n",
      "\t65.61% Cascade-Correlation (5 hidden nodes)\n",
      "\t59.2%  C4.5\n",
      "\t32.57% Linear Discriminate Analysis\n",
      "\t62.46% k=5 Nearest Neighbour\n",
      "\n",
      "\n",
      "4. Relevant Information Paragraph:\n",
      "\n",
      "   Predicting the age of abalone from physical measurements.  The age of\n",
      "   abalone is determined by cutting the shell through the cone, staining it,\n",
      "   and counting the number of rings through a microscope -- a boring and\n",
      "   time-consuming task.  Other measurements, which are easier to obtain, are\n",
      "   used to predict the age.  Further information, such as weather patterns\n",
      "   and location (hence food availability) may be required to solve the problem.\n",
      "\n",
      "   From the original data examples with missing values were removed (the\n",
      "   majority having the predicted value missing), and the ranges of the\n",
      "   continuous values have been scaled for use with an ANN (by dividing by 200).\n",
      "\n",
      "   Data comes from an original (non-machine-learning) study:\n",
      "\n",
      "\tWarwick J Nash, Tracy L Sellers, Simon R Talbot, Andrew J Cawthorn and\n",
      "\tWes B Ford (1994) \"The Population Biology of Abalone (_Haliotis_\n",
      "\tspecies) in Tasmania. I. Blacklip Abalone (_H. rubra_) from the North\n",
      "\tCoast and Islands of Bass Strait\", Sea Fisheries Division, Technical\n",
      "\tReport No. 48 (ISSN 1034-3288)\n",
      "\n",
      "\n",
      "5. Number of Instances: 4177\n",
      "\n",
      "\n",
      "6. Number of Attributes: 8\n",
      "\n",
      "\n",
      "7. Attribute information:\n",
      "\n",
      "   Given is the attribute name, attribute type, the measurement unit and a\n",
      "   brief description.  The number of rings is the value to predict: either\n",
      "   as a continuous value or as a classification problem.\n",
      "\n",
      "\tName\t\tData Type\tMeas.\tDescription\n",
      "\t----\t\t---------\t-----\t-----------\n",
      "\tSex\t\tnominal\t\t\tM, F, and I (infant)\n",
      "\tLength\t\tcontinuous\tmm\tLongest shell measurement\n",
      "\tDiameter\tcontinuous\tmm\tperpendicular to length\n",
      "\tHeight\t\tcontinuous\tmm\twith meat in shell\n",
      "\tWhole weight\tcontinuous\tgrams\twhole abalone\n",
      "\tShucked weight\tcontinuous\tgrams\tweight of meat\n",
      "\tViscera weight\tcontinuous\tgrams\tgut weight (after bleeding)\n",
      "\tShell weight\tcontinuous\tgrams\tafter being dried\n",
      "\tRings\t\tinteger\t\t\t+1.5 gives the age in years\n",
      "\n",
      "   Statistics for numeric domains:\n",
      "\n",
      "\t\tLength\tDiam\tHeight\tWhole\tShucked\tViscera\tShell\tRings\n",
      "\tMin\t0.075\t0.055\t0.000\t0.002\t0.001\t0.001\t0.002\t    1\n",
      "\tMax\t0.815\t0.650\t1.130\t2.826\t1.488\t0.760\t1.005\t   29\n",
      "\tMean\t0.524\t0.408\t0.140\t0.829\t0.359\t0.181\t0.239\t9.934\n",
      "\tSD\t0.120\t0.099\t0.042\t0.490\t0.222\t0.110\t0.139\t3.224\n",
      "\tCorrel\t0.557\t0.575\t0.557\t0.540\t0.421\t0.504\t0.628\t  1.0\n",
      "\n",
      "\n",
      "8. Missing Attribute Values: None\n",
      "\n",
      "\n",
      "9. Class Distribution:\n",
      "\n",
      "\tClass\tExamples\n",
      "\t-----\t--------\n",
      "\t1\t1\n",
      "\t2\t1\n",
      "\t3\t15\n",
      "\t4\t57\n",
      "\t5\t115\n",
      "\t6\t259\n",
      "\t7\t391\n",
      "\t8\t568\n",
      "\t9\t689\n",
      "\t10\t634\n",
      "\t11\t487\n",
      "\t12\t267\n",
      "\t13\t203\n",
      "\t14\t126\n",
      "\t15\t103\n",
      "\t16\t67\n",
      "\t17\t58\n",
      "\t18\t42\n",
      "\t19\t32\n",
      "\t20\t26\n",
      "\t21\t14\n",
      "\t22\t6\n",
      "\t23\t9\n",
      "\t24\t2\n",
      "\t25\t1\n",
      "\t26\t1\n",
      "\t27\t2\n",
      "\t29\t1\n",
      "\t-----\t----\n",
      "\tTotal\t4177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('abalone.names') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6bdc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.07</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0          0.15     15  \n",
       "1          0.07      7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.close()\n",
    "\n",
    "abalones = pd.read_csv('abalone.data', names = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings'])\n",
    "abalones.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ac2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "columns_to_encode = ['Sex']\n",
    "columns_to_scale = ['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ohe    = OneHotEncoder(sparse=False)\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, columns_to_scale),\n",
    "        (\"cat\", categorical_transformer, columns_to_encode),\n",
    "    ]\n",
    ")\n",
    "\n",
    "predictors_df = abalones.drop('Rings', axis = 1)\n",
    "predictors = preprocessor.fit_transform(predictors_df)\n",
    "target = abalones['Rings']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=42)\n",
    "\n",
    "n_cols = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, activation = 'relu', input_shape = (n_cols, )))\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e99a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 3ms/step - loss: 4.6816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b9debcc2e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9efea50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3341, 10), (836, 10), (3341,), (836,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4bb5c",
   "metadata": {},
   "source": [
    "Here I played a bit with the model manually: adding one more layer or adding more neurons per layer did not improve the performance any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709bf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdcc09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of the prediction is: 5.67324539510749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "mse = MSE(predictions, y_test)\n",
    "\n",
    "print(f'The mean squared error of the prediction is: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa4da0",
   "metadata": {},
   "source": [
    "### 4.\tWrite another algorithm to predict the same result as the previous question using either KNN or logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371b0d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "             estimator=KNeighborsRegressor(),\n",
       "             param_grid=[{'n_neighbors': [2, 3, 4, 5, 6],\n",
       "                          'weights': ['uniform', 'distance']}],\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "neigh = KNeighborsRegressor()\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define candidate hyperparameters\n",
    "parameters = [{'n_neighbors': [2,3,4,5,6], 'weights': ['uniform','distance']}]\n",
    "\n",
    "# Search for best hyperparameters\n",
    "grid = GridSearchCV(estimator=neigh, param_grid=parameters, cv=kfold, scoring='r2')\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3e931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4900224988507879\n",
      "KNeighborsRegressor(n_neighbors=6, weights='distance')\n",
      "{'n_neighbors': 6, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# Get the results\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55d2ebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of the prediction is: 5.227363651926104\n"
     ]
    }
   ],
   "source": [
    "neigh_best = KNeighborsRegressor(n_neighbors = 6, weights = 'distance')\n",
    "\n",
    "neigh_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = neigh_best.predict(X_test)\n",
    "\n",
    "mse = MSE(y_pred, y_test)\n",
    "\n",
    "print(f'The mean squared error of the prediction is: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01503d7",
   "metadata": {},
   "source": [
    "### 5.\tCreate a neural network using pytorch to predict the same result as question 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4dad692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "relu = nn.ReLU()\n",
    "\n",
    "class REG_Model(nn.Module):\n",
    "    def __init__(self, input_features = 10, hidden1 = 500, hidden2 = 500, out_features = 1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_features, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply activation functions\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62510a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([3341])) that is different to the input size (torch.Size([3341, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 78.01948547363281\n",
      "Epoch number: 41 with loss: 10.637167930603027\n",
      "Epoch number: 81 with loss: 10.329761505126953\n",
      "Epoch number: 121 with loss: 10.295011520385742\n",
      "Epoch number: 161 with loss: 10.290472030639648\n"
     ]
    }
   ],
   "source": [
    "reg = REG_Model()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(reg.parameters(), lr = 0.003)\n",
    "\n",
    "#criterions = [nn.L1Loss(), nn.MSELoss()]\n",
    "\n",
    "final_loss = []\n",
    "\n",
    "X_torch_train = torch.FloatTensor(X_train) \n",
    "X_torch_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_torch_train = torch.FloatTensor(y_train)\n",
    "\n",
    "for epoch in range(200): # loop over the dataset multiple times        \n",
    "        #forward + backward + optimize\n",
    "        outputs = reg(X_torch_train)\n",
    "        loss = criterion(outputs, y_torch_train)\n",
    "        final_loss.append(loss)\n",
    "    \n",
    "        if epoch % 40 == 1:\n",
    "            print(f'Epoch number: {epoch} with loss: {loss}')\n",
    "        \n",
    "        optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518a183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3341, 10]), torch.Size([3341]), torch.Size([836, 10]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_torch_train.shape, y_torch_train.shape, X_torch_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd1d0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad(): #decreases memory consumption\n",
    "    for i, data in enumerate(X_torch_test):\n",
    "        prediction = reg(data)\n",
    "        y_pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f75fab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of the prediction is: 10.83816372437589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:738: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:738: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py:90: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "mse = MSE(y_pred, y_test)\n",
    "\n",
    "print(f'The mean squared error of the prediction is: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85eae4",
   "metadata": {},
   "source": [
    "### 6.\tCompare the performance of the neural networks to the other model you created. Which performed better? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b747a",
   "metadata": {},
   "source": [
    "Keras and KNR performed about the same (KNR was optimized though, while the model built in KERAS was optimized only in terms of increasing the neuron count manually until the loss improved), order changing upon subsequent runs with different training sets.\n",
    "\n",
    "Torch though performed worse and stopped improving after epoch 80, even though I set the number of layers, neurons per layer, loss function and optimizer the same as in KERAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9458384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
