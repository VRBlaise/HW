{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "diabetes_df = pd.read_csv(\"../WEEK16/diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "# #Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n",
    "# print(X_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9314,  2.0179,  0.7807,  ...,  0.4315, -0.3748,  0.6321],\n",
      "        [ 0.6326, -1.1486,  0.4654,  ..., -0.1198, -0.2942,  0.7170],\n",
      "        [-0.5625, -0.4769, -0.2703,  ..., -0.2096,  2.7452,  0.0381],\n",
      "        ...,\n",
      "        [-0.8613, -0.7648,  0.0450,  ...,  0.7648, -0.7838, -0.3014],\n",
      "        [ 0.6326,  2.2099,  1.2010,  ...,  0.4315, -0.6047,  2.7537],\n",
      "        [ 0.0351,  0.7385, -0.5856,  ..., -0.3378, -0.5778,  0.2927]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #where the activation functions are\n",
    "\n",
    "#create tensors = matrices \n",
    "X_train = torch.FloatTensor(X_train) \n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#artificial neural network\n",
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=8,hidden1=20,hidden2=20,out_features=2):\n",
    "        super().__init__() #super is a computed indirect reference. So, it isolates changes\n",
    "        # and makes sure that children in the layers of multiple inheritence are calling\n",
    "        #the right parents\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#create instance of model\n",
    "ann = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(ann.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 0.6474701762199402\n",
      "Epoch number: 11 with loss: 0.5270779132843018\n",
      "Epoch number: 21 with loss: 0.45391377806663513\n",
      "Epoch number: 31 with loss: 0.4234801232814789\n",
      "Epoch number: 41 with loss: 0.39819812774658203\n",
      "Epoch number: 51 with loss: 0.3721073269844055\n",
      "Epoch number: 61 with loss: 0.3437724709510803\n",
      "Epoch number: 71 with loss: 0.31378453969955444\n",
      "Epoch number: 81 with loss: 0.28582650423049927\n",
      "Epoch number: 91 with loss: 0.25994443893432617\n",
      "Epoch number: 101 with loss: 0.23771321773529053\n",
      "Epoch number: 111 with loss: 0.21422426402568817\n",
      "Epoch number: 121 with loss: 0.19071198999881744\n",
      "Epoch number: 131 with loss: 0.17592614889144897\n",
      "Epoch number: 141 with loss: 0.15840473771095276\n",
      "Epoch number: 151 with loss: 0.14416688680648804\n",
      "Epoch number: 161 with loss: 0.1284773200750351\n",
      "Epoch number: 171 with loss: 0.11511360853910446\n",
      "Epoch number: 181 with loss: 0.1032460555434227\n",
      "Epoch number: 191 with loss: 0.09050065279006958\n",
      "Epoch number: 201 with loss: 0.07899939268827438\n",
      "Epoch number: 211 with loss: 0.06843950599431992\n",
      "Epoch number: 221 with loss: 0.06023962423205376\n",
      "Epoch number: 231 with loss: 0.05297290161252022\n",
      "Epoch number: 241 with loss: 0.046193886548280716\n",
      "Epoch number: 251 with loss: 0.040512166917324066\n",
      "Epoch number: 261 with loss: 0.03573194518685341\n",
      "Epoch number: 271 with loss: 0.030343007296323776\n",
      "Epoch number: 281 with loss: 0.025649892166256905\n",
      "Epoch number: 291 with loss: 0.02229877933859825\n",
      "Epoch number: 301 with loss: 0.019634978845715523\n",
      "Epoch number: 311 with loss: 0.017500709742307663\n",
      "Epoch number: 321 with loss: 0.015575265511870384\n",
      "Epoch number: 331 with loss: 0.013922945596277714\n",
      "Epoch number: 341 with loss: 0.012485302053391933\n",
      "Epoch number: 351 with loss: 0.011278198100626469\n",
      "Epoch number: 361 with loss: 0.010249490849673748\n",
      "Epoch number: 371 with loss: 0.009330892004072666\n",
      "Epoch number: 381 with loss: 0.008531495928764343\n",
      "Epoch number: 391 with loss: 0.00782841444015503\n",
      "Epoch number: 401 with loss: 0.007228739559650421\n",
      "Epoch number: 411 with loss: 0.006688510067760944\n",
      "Epoch number: 421 with loss: 0.006167788989841938\n",
      "Epoch number: 431 with loss: 0.005711977370083332\n",
      "Epoch number: 441 with loss: 0.0053141359239816666\n",
      "Epoch number: 451 with loss: 0.004954098723828793\n",
      "Epoch number: 461 with loss: 0.004621017724275589\n",
      "Epoch number: 471 with loss: 0.004316351842135191\n",
      "Epoch number: 481 with loss: 0.0040358975529670715\n",
      "Epoch number: 491 with loss: 0.0037874558474868536\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = ann.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss}')\n",
    "        \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() \n",
    "    optimizer.step() #perform one optimization step each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad(): #decreases memory consumption\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = ann(data)\n",
    "        y_pred.append(prediction.argmax()) #returns index with max element in each prediction set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78       150\n",
      "           1       0.59      0.51      0.54        81\n",
      "\n",
      "    accuracy                           0.70       231\n",
      "   macro avg       0.67      0.66      0.66       231\n",
      "weighted avg       0.69      0.70      0.70       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.0319, -5.4252])\n",
      "tensor([-2.6168,  1.3062])\n",
      "tensor([ 2.7535, -4.9109])\n",
      "tensor([ 0.2514, -1.6017])\n",
      "tensor([ 10.2738, -12.3366])\n",
      "tensor([ 11.4650, -14.2435])\n",
      "tensor([-1.3114, -3.3055])\n",
      "tensor([-9.7475,  7.0411])\n",
      "tensor([ 26.6600, -26.6981])\n",
      "tensor([-0.1213, -2.8400])\n",
      "tensor([ 15.4304, -15.7240])\n",
      "tensor([-9.3898, -1.8246])\n",
      "tensor([  9.3752, -10.7215])\n",
      "tensor([ 2.9755, -4.7473])\n",
      "tensor([-4.2856,  2.1033])\n",
      "tensor([-11.7091,   8.8094])\n",
      "tensor([-3.9440, -0.6266])\n",
      "tensor([ 7.1054, -8.0366])\n",
      "tensor([  8.7246, -10.4487])\n",
      "tensor([ 6.7393, -8.1200])\n",
      "tensor([-4.5387,  2.7633])\n",
      "tensor([-0.0896, -3.5822])\n",
      "tensor([-1.5577, -2.6846])\n",
      "tensor([ 22.5015, -22.7031])\n",
      "tensor([-9.7996,  7.3229])\n",
      "tensor([ 5.1007, -6.6488])\n",
      "tensor([ 2.5190, -5.1220])\n",
      "tensor([ 2.2193, -2.2844])\n",
      "tensor([ 1.6401, -1.7862])\n",
      "tensor([-5.5820,  1.5395])\n",
      "tensor([-12.2231,   6.9077])\n",
      "tensor([ 5.1005, -5.9326])\n",
      "tensor([-1.6240, -0.7265])\n",
      "tensor([  8.5741, -12.6062])\n",
      "tensor([ 38.3464, -35.0321])\n",
      "tensor([-7.6075,  5.0630])\n",
      "tensor([ 3.4063, -4.0645])\n",
      "tensor([ 0.3389, -2.5697])\n",
      "tensor([-3.5105,  1.9932])\n",
      "tensor([-1.9418, -2.7237])\n",
      "tensor([ 15.5604, -15.6080])\n",
      "tensor([ 0.6750, -3.5966])\n",
      "tensor([  2.3094, -13.4374])\n",
      "tensor([  8.8946, -11.0677])\n",
      "tensor([ 12.6911, -12.8679])\n",
      "tensor([ 0.5559, -1.3858])\n",
      "tensor([ 2.4969, -3.7820])\n",
      "tensor([-14.4940,  10.1159])\n",
      "tensor([ 12.6961, -18.9496])\n",
      "tensor([ 3.9932, -5.8316])\n",
      "tensor([ 12.8987, -13.8328])\n",
      "tensor([ 18.7097, -17.8698])\n",
      "tensor([-2.1514,  0.2567])\n",
      "tensor([ 16.2682, -18.9423])\n",
      "tensor([-1.0156, -0.6092])\n",
      "tensor([ 14.4060, -14.5677])\n",
      "tensor([  8.3470, -13.9561])\n",
      "tensor([-3.9160,  1.2395])\n",
      "tensor([-2.6451, -2.7683])\n",
      "tensor([ 1.1796, -3.2625])\n",
      "tensor([ 25.3041, -25.6783])\n",
      "tensor([ 4.1468, -5.6537])\n",
      "tensor([ 1.7600, -5.3068])\n",
      "tensor([ 0.8077, -1.7704])\n",
      "tensor([  5.0792, -12.9447])\n",
      "tensor([ 5.3170, -6.7001])\n",
      "tensor([ 1.6474, -6.6299])\n",
      "tensor([ 6.1887, -7.4332])\n",
      "tensor([ 5.2751, -6.8606])\n",
      "tensor([-0.1693, -1.1551])\n",
      "tensor([ 1.9251, -3.7082])\n",
      "tensor([-4.1566,  2.3341])\n",
      "tensor([ 13.6658, -27.4868])\n",
      "tensor([ 20.5382, -34.9304])\n",
      "tensor([ 10.7161, -14.0850])\n",
      "tensor([  4.3449, -11.9822])\n",
      "tensor([ 4.3255, -3.9733])\n",
      "tensor([-5.0215,  2.6486])\n",
      "tensor([ 5.8145, -7.9007])\n",
      "tensor([-1.4506, -1.5942])\n",
      "tensor([ 6.4891, -8.8716])\n",
      "tensor([ 6.6193, -8.3519])\n",
      "tensor([ 16.4522, -17.9883])\n",
      "tensor([ 46.7463, -46.6065])\n",
      "tensor([ 5.2440, -6.2107])\n",
      "tensor([ 3.0052, -4.7986])\n",
      "tensor([-12.9611,   4.3837])\n",
      "tensor([-3.2234,  1.5034])\n",
      "tensor([-5.5300,  3.3629])\n",
      "tensor([ 2.0932, -4.6265])\n",
      "tensor([-17.3387,  14.1482])\n",
      "tensor([-18.2364,  13.2306])\n",
      "tensor([ 22.2759, -22.6594])\n",
      "tensor([-4.8935,  2.4578])\n",
      "tensor([ 1.9021, -6.8105])\n",
      "tensor([ 15.7547, -16.7586])\n",
      "tensor([-0.5561, -1.4610])\n",
      "tensor([ 21.6104, -22.1999])\n",
      "tensor([-1.7918, -1.0475])\n",
      "tensor([-7.2507,  5.0793])\n",
      "tensor([ 6.8232, -7.4219])\n",
      "tensor([ 24.7806, -24.5729])\n",
      "tensor([-0.9240, -2.2011])\n",
      "tensor([-2.0906, -1.7001])\n",
      "tensor([ 3.0446, -5.0690])\n",
      "tensor([ 22.5220, -23.0201])\n",
      "tensor([ 36.2956, -37.2704])\n",
      "tensor([-21.5051,  18.1283])\n",
      "tensor([ 22.3253, -23.6833])\n",
      "tensor([ 0.7568, -1.9157])\n",
      "tensor([ 10.3729, -15.1215])\n",
      "tensor([-2.4635,  0.9070])\n",
      "tensor([ 8.3368, -9.7902])\n",
      "tensor([-9.5290,  7.3253])\n",
      "tensor([ 2.9302, -8.5378])\n",
      "tensor([ 0.6713, -2.5499])\n",
      "tensor([ 7.2025, -5.9876])\n",
      "tensor([-4.0891, -0.6531])\n",
      "tensor([ 6.0506, -6.2575])\n",
      "tensor([-8.9183,  3.1571])\n",
      "tensor([ 73.5975, -78.7240])\n",
      "tensor([-4.3779,  2.0540])\n",
      "tensor([-7.2495,  4.9350])\n",
      "tensor([-1.3638, -0.6436])\n",
      "tensor([-9.7308,  5.8425])\n",
      "tensor([ 19.6891, -18.5572])\n",
      "tensor([-23.5115,  19.4991])\n",
      "tensor([ 5.8267, -7.9372])\n",
      "tensor([-7.9727,  5.4655])\n",
      "tensor([  8.9353, -11.0063])\n",
      "tensor([ 32.7005, -31.5911])\n",
      "tensor([  9.1931, -10.0551])\n",
      "tensor([-3.2159,  0.3763])\n",
      "tensor([-2.6892,  1.1236])\n",
      "tensor([ 0.6648, -1.5677])\n",
      "tensor([-5.1197,  2.0532])\n",
      "tensor([ 20.6764, -19.4971])\n",
      "tensor([ 33.6148, -46.1409])\n",
      "tensor([ 11.7460, -12.7172])\n",
      "tensor([-10.6398,   2.0827])\n",
      "tensor([ 0.4833, -3.3558])\n",
      "tensor([ 7.2325, -7.6005])\n",
      "tensor([-9.5685,  5.3301])\n",
      "tensor([ 1.4498, -3.1819])\n",
      "tensor([ 5.7920, -5.3254])\n",
      "tensor([ 8.0313, -9.5571])\n",
      "tensor([ 44.6546, -41.6350])\n",
      "tensor([ 16.6249, -17.9131])\n",
      "tensor([ 0.2558, -3.2705])\n",
      "tensor([ 7.1708, -8.0886])\n",
      "tensor([ 10.9562, -13.1318])\n",
      "tensor([ 4.2985, -5.7209])\n",
      "tensor([ 0.7425, -2.6440])\n",
      "tensor([  7.9947, -10.3003])\n",
      "tensor([-2.8918, -0.3734])\n",
      "tensor([ 2.2452, -3.0485])\n",
      "tensor([-3.9457,  2.8992])\n",
      "tensor([ 2.7478, -2.9008])\n",
      "tensor([ 19.9112, -19.8811])\n",
      "tensor([  9.3465, -10.0576])\n",
      "tensor([-3.8262,  2.4460])\n",
      "tensor([ 17.6492, -16.9879])\n",
      "tensor([ 22.0607, -21.4987])\n",
      "tensor([-3.1211, -1.1382])\n",
      "tensor([-3.6706, -8.1647])\n",
      "tensor([ 6.4056, -7.2083])\n",
      "tensor([  9.4220, -12.4569])\n",
      "tensor([-3.8375,  1.2167])\n",
      "tensor([-11.8114,   9.5008])\n",
      "tensor([  8.6352, -10.2417])\n",
      "tensor([ 42.6433, -40.3823])\n",
      "tensor([-3.7729,  0.9450])\n",
      "tensor([ 2.4267, -5.9427])\n",
      "tensor([  7.6434, -10.0357])\n",
      "tensor([ 10.3630, -14.6158])\n",
      "tensor([ 4.7310, -6.8660])\n",
      "tensor([ 1.9273, -3.1430])\n",
      "tensor([  9.6335, -11.3620])\n",
      "tensor([-17.4038,  12.8878])\n",
      "tensor([-0.8688, -0.1215])\n",
      "tensor([-3.5197, -1.6434])\n",
      "tensor([-3.6948,  3.1264])\n",
      "tensor([  8.2868, -10.2195])\n",
      "tensor([ 4.2843, -7.1984])\n",
      "tensor([-1.7562,  0.1083])\n",
      "tensor([-7.5459,  3.0164])\n",
      "tensor([ 1.4801, -1.1627])\n",
      "tensor([ 6.8001, -9.0372])\n",
      "tensor([ 12.8796, -14.3957])\n",
      "tensor([-0.2282, -1.2362])\n",
      "tensor([-4.5264,  1.1891])\n",
      "tensor([ 45.2694, -46.5248])\n",
      "tensor([-9.6118,  8.3838])\n",
      "tensor([ 1.7833, -3.8704])\n",
      "tensor([ 10.5291, -13.7960])\n",
      "tensor([ 2.5990, -4.9636])\n",
      "tensor([-1.3242,  0.1367])\n",
      "tensor([-3.2788, -0.6539])\n",
      "tensor([ 0.0876, -3.8505])\n",
      "tensor([ 6.9069, -7.9526])\n",
      "tensor([ 28.3146, -26.7953])\n",
      "tensor([ -1.7275, -10.8165])\n",
      "tensor([ 2.0836, -8.3594])\n",
      "tensor([ 2.1285, -2.2597])\n",
      "tensor([ 5.1263, -5.0618])\n",
      "tensor([-4.0364,  1.5147])\n",
      "tensor([ 2.8375, -3.8304])\n",
      "tensor([-10.4591,   8.6377])\n",
      "tensor([ 0.7016, -9.5436])\n",
      "tensor([ 48.4800, -50.1568])\n",
      "tensor([ 35.4645, -33.4702])\n",
      "tensor([ 5.3141, -5.2953])\n",
      "tensor([-6.5498,  4.1560])\n",
      "tensor([ 2.8787, -4.1044])\n",
      "tensor([ 1.3860, -6.7656])\n",
      "tensor([ 16.8175, -27.1029])\n",
      "tensor([-10.3629,   7.7380])\n",
      "tensor([-1.3110, -2.8275])\n",
      "tensor([ 6.1406, -7.2498])\n",
      "tensor([-3.6343,  0.3908])\n",
      "tensor([ 0.2721, -1.3122])\n",
      "tensor([-4.9856,  0.8133])\n",
      "tensor([  8.2330, -12.7242])\n",
      "tensor([ 33.2681, -30.9492])\n",
      "tensor([ 38.8943, -41.0441])\n",
      "tensor([-6.1048,  4.0063])\n",
      "tensor([-0.1122, -0.9104])\n",
      "tensor([-6.0104,  2.7254])\n",
      "tensor([ 1.0172, -3.6345])\n",
      "tensor([-5.3213, -0.6774])\n",
      "tensor([ 7.1538, -7.1768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): #decreases memory consumption\n",
    "    for i, data in enumerate(X_test):\n",
    "        print(ann(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2220,  0.2730,  0.4480,  ..., -0.4052, -0.4010,  0.1271],\n",
       "        [ 0.0710, -0.3540, -0.2502,  ...,  0.3897,  1.4114, -0.5597],\n",
       "        [ 2.1222,  0.6611,  1.2460,  ...,  0.6133, -0.6158,  1.5866],\n",
       "        ...,\n",
       "        [ 2.7082, -0.4436,  0.1488,  ...,  0.6133, -0.8525,  1.0715],\n",
       "        [-0.5150,  1.1089,  1.0465,  ..., -0.0077,  1.1000,  2.8743],\n",
       "        [-0.8080, -0.1749,  0.0490,  ...,  0.3649,  0.2405, -0.0446]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if epoch == 500 and loss < 0.1:\n",
    "            #predictions \n",
    "            y_pred = []\n",
    "            y_pred_prob = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(X_test):\n",
    "                    prediction = ann(data)\n",
    "                    y_pred.append(prediction.argmax())\n",
    "                    #prob = F.softmax(ann(data), dim=1)\n",
    "                    #top_p, top_class = prob.topk(1, dim = 1)\n",
    "                    #y_pred_prob.append(top_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
