{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb61f50",
   "metadata": {},
   "source": [
    "### Regular Expressions (RegEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01440c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194e7d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('abc', 'abcdef') # pattern as first argument, string as second argument, returns a match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88cf770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('bc', 'abcdf') # no match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4d9246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 3), match='bc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('bc', 'abcdf') # finds it inside the word, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f9b7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_regex = '\\w+' # special code for word\n",
    "re.match(word_regex, 'hi there!') # mathes the first word it finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad18e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '2', 'cats']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_regex = '\\d'\n",
    "space_regex = '\\s'\n",
    "wildcard_regex = '.*'\n",
    "brackets_regex = r\"\\[.*\\]\" # finds anyhting in brakets backslash are escape characters, so that bracket means bracket\n",
    "script_regex = r\"[\\w\\s]+:\" # finds the script notation and the word or space before\n",
    "greedy_regex = 'a+' # gets aaaaas, can be 'a*' also\n",
    "nospace_regex = '\\S' # uppercase version negates them\n",
    "lowercase_regex = '[a-z]' # looks for group of letters\n",
    "# or is |\n",
    "# group is defined by ()\n",
    "# explicit character ranges are defined by []\n",
    "# usefull: split, findall, search, match\n",
    "# pattern goes first, string second\n",
    "# returns an iterator, string, match object\n",
    "\n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'He has 2 cats.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb373f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [A-Z] A to Z included\n",
    "# (A-Z) A and - and Z\n",
    "# [A-Za-z] A to Z and a to z, so both lower and upper\n",
    "# [A-Za-z\\-\\.] the above and - and .\n",
    "# (\\s+|,) spaces or coma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d357ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['split', 'on', 'spaces']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'split on spaces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7252fae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', \"Let's write RegEx!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c935c1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\s+', \"Let's write RegEx!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9298c1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 't', 's', 'w', 'r', 'i', 't', 'e', 'e', 'g', 'x']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z]\", \"Let's write RegEx!\") # finds only the lowercase ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b26f496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L', 'e', 't', 's', 'w', 'r', 'i', 't', 'e', 'R', 'e', 'g', 'E', 'x']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w', \"Let's write RegEx!\") # apparently these are all letters, no spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58d52810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b251f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "pattern = '(\\\\w+|#\\\\d|\\\\?|!)'\n",
    "print(re.findall(pattern, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61597ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "pattern1 = '(\\w+|#\\d|\\?|!)' # why does this work at all? I am not escaping backslash here...\n",
    "print(re.findall(pattern1, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2839e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "pattern2 = r'(\\w+|#\\d|\\?|!)'\n",
    "print(re.findall(pattern2, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73023202",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = r\"#\\w+\"\n",
    "mentiones_hashtags = r\"([@#]\\w+)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1088d9",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4e7beed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\balazs.varga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize # tokens are smaller units of language\n",
    "word_tokenize('Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f32e0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenize\n",
    "# regexp_tokenize is based on a regular expression pattern\n",
    "# TweetTokenizer a special class just for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1afa84cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'effrenata', ']', 'iste', 'O', 'teneri', 'ac', 'quem', 'convocaveris', 'furorem', 'Patere', 'Ad', 'iam', 'sese', 'facere', 'rei', 'diu', 'locus', 'voltusque', 'ignorare', 'iactabit', 'in', 'duci', 'Quo', 'moverunt', 'consilia', 'pridem', 'intellegit', 'nihil', 'consulis', 'sentis', 'vitemus', 'Quid', 'nostrum', ',', 'patientia', 'videt', 'satis', 'arbitraris', 'designat', 'publici', 'fortes', 'Nos', 'senatum', 'populi', 'urbis', 'fit', 'nos', 'quam', 'timor', 'particeps', 'vides', 'constrictam', 'vero', 'tandem', 'hic', 'Consul', 'non', 'Nihilne', 'ceperis', 'si', 'quid', '[', 'viri', 'mores', 'audacia', 'tuam', 'bonorum', 'oculis', '?', 'Vivit', 'Palati', 'Senatus', 'consilii', 'vivit', 'etiam', 'omnium', 'ad', 'quos', 'scientia', 'tua', 'coniurationem', 'oportebat', 'eludet', 'habendi', 'horum', 'nocte', 'istius', 'concursus', 'machinaris', 'fueris', 'usque', 'venit', 'nostra', 'nocturnum', 'tu', 'autem', '!', 'ubi', 'omnes', 'ora', '.', 'te', 'et', 'o', 'senatus', 'praesidium', 'egeris', 'conferri', 'pestem', 'tuus', 'immo', 'notat', 'mortem', 'vigiliae', 'haec', 'proxima', 'tamen', 'furor', 'unum', 'iussu', 'videmur', 'finem', 'publicae', 'superiore', ';', 'tempora', 'Catilina', 'quemque', 'abutere', 'tela', 'caedem', 'munitissimus'}\n"
     ]
    }
   ],
   "source": [
    "scene_one = \"\"\"Quo usque tandem abutere, Catilina, patientia nostra? quam diu etiam furor iste tuus nos eludet? \n",
    "quem ad finem sese effrenata iactabit audacia? Nihilne te nocturnum praesidium Palati, nihil urbis vigiliae, \n",
    "nihil timor populi, nihil concursus bonorum omnium, nihil hic munitissimus habendi senatus locus, nihil horum ora\n",
    "voltusque moverunt? Patere tua consilia non sentis, constrictam iam horum omnium scientia teneri coniurationem tuam\n",
    "non vides? Quid proxima, quid superiore nocte egeris, ubi fueris, quos convocaveris, quid consilii ceperis, quem\n",
    "nostrum ignorare arbitraris? O tempora, o mores! Senatus haec intellegit. Consul videt; hic tamen vivit. Vivit?\n",
    "immo vero etiam in senatum venit, fit publici consilii particeps, notat et designat oculis ad caedem unum quemque nostrum.\n",
    "Nos autem fortes viri satis facere rei publicae videmur, si istius furorem ac tela vitemus. Ad mortem te, Catilina,\n",
    "duci iussu consulis iam pridem oportebat, in te conferri pestem, quam tu in nos [omnes iam diu] machinaris.\"\"\"\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(scene_one)\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67bdc9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "word_lengths = [len(w) for w in unique_tokens]\n",
    "lengths_set = set(word_lengths)\n",
    "rangend = max(lengths_set)\n",
    "\n",
    "_= plt.figure(figsize = (12, 3))\n",
    "_= plt.hist(word_lengths, bins=np.arange(1, 15)-0.5)\n",
    "_= plt.xticks(np.arange(1,14))\n",
    "_= plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cc353",
   "metadata": {},
   "source": [
    "Bincount is more efficient at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2ab0fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = np.bincount(word_lengths)\n",
    "\n",
    "# Switching to the OO-interface. You can do all of this with \"plt\" as well.\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(rangend+1), counts, width=0.8, align='center')\n",
    "ax.set(xticks=range(1, rangend+1), xlim=[0, rangend+1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e547ed",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ac84cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8c7fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 21),\n",
       " ('?', 7),\n",
       " ('nihil', 5),\n",
       " ('.', 5),\n",
       " ('nos', 3),\n",
       " ('ad', 3),\n",
       " ('te', 3),\n",
       " ('iam', 3),\n",
       " ('quid', 3),\n",
       " ('in', 3),\n",
       " ('catilina', 2),\n",
       " ('quam', 2),\n",
       " ('diu', 2),\n",
       " ('etiam', 2),\n",
       " ('quem', 2),\n",
       " ('omnium', 2),\n",
       " ('hic', 2),\n",
       " ('senatus', 2),\n",
       " ('horum', 2),\n",
       " ('non', 2),\n",
       " ('consilii', 2),\n",
       " ('nostrum', 2),\n",
       " ('o', 2),\n",
       " ('vivit', 2),\n",
       " ('quo', 1),\n",
       " ('usque', 1),\n",
       " ('tandem', 1),\n",
       " ('abutere', 1),\n",
       " ('patientia', 1),\n",
       " ('nostra', 1),\n",
       " ('furor', 1),\n",
       " ('iste', 1),\n",
       " ('tuus', 1),\n",
       " ('eludet', 1),\n",
       " ('finem', 1),\n",
       " ('sese', 1),\n",
       " ('effrenata', 1),\n",
       " ('iactabit', 1),\n",
       " ('audacia', 1),\n",
       " ('nihilne', 1),\n",
       " ('nocturnum', 1),\n",
       " ('praesidium', 1),\n",
       " ('palati', 1),\n",
       " ('urbis', 1),\n",
       " ('vigiliae', 1),\n",
       " ('timor', 1),\n",
       " ('populi', 1),\n",
       " ('concursus', 1),\n",
       " ('bonorum', 1),\n",
       " ('munitissimus', 1),\n",
       " ('habendi', 1),\n",
       " ('locus', 1),\n",
       " ('ora', 1),\n",
       " ('voltusque', 1),\n",
       " ('moverunt', 1),\n",
       " ('patere', 1),\n",
       " ('tua', 1),\n",
       " ('consilia', 1),\n",
       " ('sentis', 1),\n",
       " ('constrictam', 1),\n",
       " ('scientia', 1),\n",
       " ('teneri', 1),\n",
       " ('coniurationem', 1),\n",
       " ('tuam', 1),\n",
       " ('vides', 1),\n",
       " ('proxima', 1),\n",
       " ('superiore', 1),\n",
       " ('nocte', 1),\n",
       " ('egeris', 1),\n",
       " ('ubi', 1),\n",
       " ('fueris', 1),\n",
       " ('quos', 1),\n",
       " ('convocaveris', 1),\n",
       " ('ceperis', 1),\n",
       " ('ignorare', 1),\n",
       " ('arbitraris', 1),\n",
       " ('tempora', 1),\n",
       " ('mores', 1),\n",
       " ('!', 1),\n",
       " ('haec', 1),\n",
       " ('intellegit', 1),\n",
       " ('consul', 1),\n",
       " ('videt', 1),\n",
       " (';', 1),\n",
       " ('tamen', 1),\n",
       " ('immo', 1),\n",
       " ('vero', 1),\n",
       " ('senatum', 1),\n",
       " ('venit', 1),\n",
       " ('fit', 1),\n",
       " ('publici', 1),\n",
       " ('particeps', 1),\n",
       " ('notat', 1),\n",
       " ('et', 1),\n",
       " ('designat', 1),\n",
       " ('oculis', 1),\n",
       " ('caedem', 1),\n",
       " ('unum', 1),\n",
       " ('quemque', 1),\n",
       " ('autem', 1),\n",
       " ('fortes', 1),\n",
       " ('viri', 1),\n",
       " ('satis', 1),\n",
       " ('facere', 1),\n",
       " ('rei', 1),\n",
       " ('publicae', 1),\n",
       " ('videmur', 1),\n",
       " ('si', 1),\n",
       " ('istius', 1),\n",
       " ('furorem', 1),\n",
       " ('ac', 1),\n",
       " ('tela', 1),\n",
       " ('vitemus', 1),\n",
       " ('mortem', 1),\n",
       " ('duci', 1),\n",
       " ('iussu', 1),\n",
       " ('consulis', 1),\n",
       " ('pridem', 1),\n",
       " ('oportebat', 1),\n",
       " ('conferri', 1),\n",
       " ('pestem', 1),\n",
       " ('tu', 1),\n",
       " ('[', 1),\n",
       " ('omnes', 1),\n",
       " (']', 1),\n",
       " ('machinaris', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(word_tokenize(scene_one.lower()))\n",
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee267c",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0473337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'bengali', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e995f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_two = \"\"\" How far will you abuse our patience, Catiline? \n",
    "For how much longer will that rage of yours make a mockery of us? \n",
    "To what point will your unbridled audacity show itself? \n",
    "Did the nocturnal garrison on the Palatine, the watch patrols of\n",
    "the city, the fear of the people, the assemblies of all the good men, this most fortified\n",
    "place of holding the senate, the faces and expressions of all these people\n",
    "not move you at all? Do you not realise that your plans lie revealed? Do you not see\n",
    "that your plot is already held in check by the knowledge of all these people? Do you\n",
    "think that any of us do not know what you did last night, what you did the night\n",
    "before, where you were, who you summoned, and what plans you made? \n",
    "O what times! O what customs! The senate understands these things;\n",
    "the consul sees these things; this man, however, lives. He lives? No\n",
    "indeed, he even comes to the senate. He even takes part in public affairs. He points\n",
    "out and designates with his eyes, individuals amongst us for slaughter. But we, brave\n",
    "men, seem to do enough for the state, if we avoid, the rage and the weapons of that\n",
    "man. You, Catiline, should have been led to death already long ago by order of the consul, \n",
    "that ruin, which you are devising against us, should have been conferred upon you.\"\"\"\n",
    "\n",
    "tokens = [w for w in word_tokenize(scene_two.lower()) if w.isalpha()] # true for alphabetical only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "651f5d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('us', 4), ('people', 3), ('senate', 3), ('catiline', 2), ('rage', 2), ('men', 2), ('plans', 2), ('already', 2), ('night', 2), ('things', 2), ('consul', 2), ('man', 2), ('lives', 2), ('even', 2), ('far', 1), ('abuse', 1), ('patience', 1), ('much', 1), ('longer', 1), ('make', 1), ('mockery', 1), ('point', 1), ('unbridled', 1), ('audacity', 1), ('show', 1), ('nocturnal', 1), ('garrison', 1), ('palatine', 1), ('watch', 1), ('patrols', 1), ('city', 1), ('fear', 1), ('assemblies', 1), ('good', 1), ('fortified', 1), ('place', 1), ('holding', 1), ('faces', 1), ('expressions', 1), ('move', 1), ('realise', 1), ('lie', 1), ('revealed', 1), ('see', 1), ('plot', 1), ('held', 1), ('check', 1), ('knowledge', 1), ('think', 1), ('know', 1), ('last', 1), ('summoned', 1), ('made', 1), ('times', 1), ('customs', 1), ('understands', 1), ('sees', 1), ('however', 1), ('indeed', 1), ('comes', 1), ('takes', 1), ('part', 1), ('public', 1), ('affairs', 1), ('points', 1), ('designates', 1), ('eyes', 1), ('individuals', 1), ('amongst', 1), ('slaughter', 1), ('brave', 1), ('seem', 1), ('enough', 1), ('state', 1), ('avoid', 1), ('weapons', 1), ('led', 1), ('death', 1), ('long', 1), ('ago', 1), ('order', 1), ('ruin', 1), ('devising', 1), ('conferred', 1), ('upon', 1)]\n"
     ]
    }
   ],
   "source": [
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "counter = Counter(no_stops)\n",
    "print(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afa73c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['far', 'abuse', 'patience', 'catiline', 'much', 'longer', 'rage', 'make', 'mockery', 'us', 'point', 'unbridled', 'audacity', 'show', 'nocturnal', 'garrison', 'palatine', 'watch', 'patrols', 'city', 'fear', 'people', 'assemblies', 'good', 'men', 'fortified', 'place', 'holding', 'senate', 'faces', 'expressions', 'people', 'move', 'realise', 'plans', 'lie', 'revealed', 'see', 'plot', 'already', 'held', 'check', 'knowledge', 'people', 'think', 'us', 'know', 'last', 'night', 'night', 'summoned', 'plans', 'made', 'times', 'customs', 'senate', 'understands', 'things', 'consul', 'sees', 'things', 'man', 'however', 'lives', 'lives', 'indeed', 'even', 'comes', 'senate', 'even', 'takes', 'part', 'public', 'affairs', 'points', 'designates', 'eyes', 'individuals', 'amongst', 'us', 'slaughter', 'brave', 'men', 'seem', 'enough', 'state', 'avoid', 'rage', 'weapons', 'man', 'catiline', 'led', 'death', 'already', 'long', 'ago', 'order', 'consul', 'ruin', 'devising', 'us', 'conferred', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df13cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\balazs.varga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "266be5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('u', 4), ('people', 3), ('senate', 3), ('catiline', 2), ('rage', 2), ('point', 2), ('men', 2), ('plan', 2), ('see', 2), ('already', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "counter = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd367b0",
   "metadata": {},
   "source": [
    "### Introduction to Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "352535b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5bcd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_documents = [scene_one, scene_two]\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs) # we begin corpus of the latin and english texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "545d4f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " ';': 3,\n",
       " '?': 4,\n",
       " 'Ad': 5,\n",
       " 'Catilina': 6,\n",
       " 'Consul': 7,\n",
       " 'Nihilne': 8,\n",
       " 'Nos': 9,\n",
       " 'O': 10,\n",
       " 'Palati': 11,\n",
       " 'Patere': 12,\n",
       " 'Quid': 13,\n",
       " 'Quo': 14,\n",
       " 'Senatus': 15,\n",
       " 'Vivit': 16,\n",
       " '[': 17,\n",
       " ']': 18,\n",
       " 'abutere': 19,\n",
       " 'ac': 20,\n",
       " 'ad': 21,\n",
       " 'arbitraris': 22,\n",
       " 'audacia': 23,\n",
       " 'autem': 24,\n",
       " 'bonorum': 25,\n",
       " 'caedem': 26,\n",
       " 'ceperis': 27,\n",
       " 'concursus': 28,\n",
       " 'conferri': 29,\n",
       " 'coniurationem': 30,\n",
       " 'consilia': 31,\n",
       " 'consilii': 32,\n",
       " 'constrictam': 33,\n",
       " 'consulis': 34,\n",
       " 'convocaveris': 35,\n",
       " 'designat': 36,\n",
       " 'diu': 37,\n",
       " 'duci': 38,\n",
       " 'effrenata': 39,\n",
       " 'egeris': 40,\n",
       " 'eludet': 41,\n",
       " 'et': 42,\n",
       " 'etiam': 43,\n",
       " 'facere': 44,\n",
       " 'finem': 45,\n",
       " 'fit': 46,\n",
       " 'fortes': 47,\n",
       " 'fueris': 48,\n",
       " 'furor': 49,\n",
       " 'furorem': 50,\n",
       " 'habendi': 51,\n",
       " 'haec': 52,\n",
       " 'hic': 53,\n",
       " 'horum': 54,\n",
       " 'iactabit': 55,\n",
       " 'iam': 56,\n",
       " 'ignorare': 57,\n",
       " 'immo': 58,\n",
       " 'in': 59,\n",
       " 'intellegit': 60,\n",
       " 'iste': 61,\n",
       " 'istius': 62,\n",
       " 'iussu': 63,\n",
       " 'locus': 64,\n",
       " 'machinaris': 65,\n",
       " 'mores': 66,\n",
       " 'mortem': 67,\n",
       " 'moverunt': 68,\n",
       " 'munitissimus': 69,\n",
       " 'nihil': 70,\n",
       " 'nocte': 71,\n",
       " 'nocturnum': 72,\n",
       " 'non': 73,\n",
       " 'nos': 74,\n",
       " 'nostra': 75,\n",
       " 'nostrum': 76,\n",
       " 'notat': 77,\n",
       " 'o': 78,\n",
       " 'oculis': 79,\n",
       " 'omnes': 80,\n",
       " 'omnium': 81,\n",
       " 'oportebat': 82,\n",
       " 'ora': 83,\n",
       " 'particeps': 84,\n",
       " 'patientia': 85,\n",
       " 'pestem': 86,\n",
       " 'populi': 87,\n",
       " 'praesidium': 88,\n",
       " 'pridem': 89,\n",
       " 'proxima': 90,\n",
       " 'publicae': 91,\n",
       " 'publici': 92,\n",
       " 'quam': 93,\n",
       " 'quem': 94,\n",
       " 'quemque': 95,\n",
       " 'quid': 96,\n",
       " 'quos': 97,\n",
       " 'rei': 98,\n",
       " 'satis': 99,\n",
       " 'scientia': 100,\n",
       " 'senatum': 101,\n",
       " 'senatus': 102,\n",
       " 'sentis': 103,\n",
       " 'sese': 104,\n",
       " 'si': 105,\n",
       " 'superiore': 106,\n",
       " 'tamen': 107,\n",
       " 'tandem': 108,\n",
       " 'te': 109,\n",
       " 'tela': 110,\n",
       " 'tempora': 111,\n",
       " 'teneri': 112,\n",
       " 'timor': 113,\n",
       " 'tu': 114,\n",
       " 'tua': 115,\n",
       " 'tuam': 116,\n",
       " 'tuus': 117,\n",
       " 'ubi': 118,\n",
       " 'unum': 119,\n",
       " 'urbis': 120,\n",
       " 'usque': 121,\n",
       " 'venit': 122,\n",
       " 'vero': 123,\n",
       " 'videmur': 124,\n",
       " 'vides': 125,\n",
       " 'videt': 126,\n",
       " 'vigiliae': 127,\n",
       " 'viri': 128,\n",
       " 'vitemus': 129,\n",
       " 'vivit': 130,\n",
       " 'voltusque': 131,\n",
       " 'But': 132,\n",
       " 'Catiline': 133,\n",
       " 'Did': 134,\n",
       " 'Do': 135,\n",
       " 'For': 136,\n",
       " 'He': 137,\n",
       " 'How': 138,\n",
       " 'No': 139,\n",
       " 'Palatine': 140,\n",
       " 'The': 141,\n",
       " 'To': 142,\n",
       " 'You': 143,\n",
       " 'a': 144,\n",
       " 'abuse': 145,\n",
       " 'affairs': 146,\n",
       " 'against': 147,\n",
       " 'ago': 148,\n",
       " 'all': 149,\n",
       " 'already': 150,\n",
       " 'amongst': 151,\n",
       " 'and': 152,\n",
       " 'any': 153,\n",
       " 'are': 154,\n",
       " 'assemblies': 155,\n",
       " 'at': 156,\n",
       " 'audacity': 157,\n",
       " 'avoid': 158,\n",
       " 'been': 159,\n",
       " 'before': 160,\n",
       " 'brave': 161,\n",
       " 'by': 162,\n",
       " 'check': 163,\n",
       " 'city': 164,\n",
       " 'comes': 165,\n",
       " 'conferred': 166,\n",
       " 'consul': 167,\n",
       " 'customs': 168,\n",
       " 'death': 169,\n",
       " 'designates': 170,\n",
       " 'devising': 171,\n",
       " 'did': 172,\n",
       " 'do': 173,\n",
       " 'enough': 174,\n",
       " 'even': 175,\n",
       " 'expressions': 176,\n",
       " 'eyes': 177,\n",
       " 'faces': 178,\n",
       " 'far': 179,\n",
       " 'fear': 180,\n",
       " 'for': 181,\n",
       " 'fortified': 182,\n",
       " 'garrison': 183,\n",
       " 'good': 184,\n",
       " 'have': 185,\n",
       " 'he': 186,\n",
       " 'held': 187,\n",
       " 'his': 188,\n",
       " 'holding': 189,\n",
       " 'how': 190,\n",
       " 'however': 191,\n",
       " 'if': 192,\n",
       " 'indeed': 193,\n",
       " 'individuals': 194,\n",
       " 'is': 195,\n",
       " 'itself': 196,\n",
       " 'know': 197,\n",
       " 'knowledge': 198,\n",
       " 'last': 199,\n",
       " 'led': 200,\n",
       " 'lie': 201,\n",
       " 'lives': 202,\n",
       " 'long': 203,\n",
       " 'longer': 204,\n",
       " 'made': 205,\n",
       " 'make': 206,\n",
       " 'man': 207,\n",
       " 'men': 208,\n",
       " 'mockery': 209,\n",
       " 'most': 210,\n",
       " 'move': 211,\n",
       " 'much': 212,\n",
       " 'night': 213,\n",
       " 'nocturnal': 214,\n",
       " 'not': 215,\n",
       " 'of': 216,\n",
       " 'on': 217,\n",
       " 'order': 218,\n",
       " 'our': 219,\n",
       " 'out': 220,\n",
       " 'part': 221,\n",
       " 'patience': 222,\n",
       " 'patrols': 223,\n",
       " 'people': 224,\n",
       " 'place': 225,\n",
       " 'plans': 226,\n",
       " 'plot': 227,\n",
       " 'point': 228,\n",
       " 'points': 229,\n",
       " 'public': 230,\n",
       " 'rage': 231,\n",
       " 'realise': 232,\n",
       " 'revealed': 233,\n",
       " 'ruin': 234,\n",
       " 'see': 235,\n",
       " 'seem': 236,\n",
       " 'sees': 237,\n",
       " 'senate': 238,\n",
       " 'should': 239,\n",
       " 'show': 240,\n",
       " 'slaughter': 241,\n",
       " 'state': 242,\n",
       " 'summoned': 243,\n",
       " 'takes': 244,\n",
       " 'that': 245,\n",
       " 'the': 246,\n",
       " 'these': 247,\n",
       " 'things': 248,\n",
       " 'think': 249,\n",
       " 'this': 250,\n",
       " 'times': 251,\n",
       " 'to': 252,\n",
       " 'unbridled': 253,\n",
       " 'understands': 254,\n",
       " 'upon': 255,\n",
       " 'us': 256,\n",
       " 'watch': 257,\n",
       " 'we': 258,\n",
       " 'weapons': 259,\n",
       " 'were': 260,\n",
       " 'what': 261,\n",
       " 'where': 262,\n",
       " 'which': 263,\n",
       " 'who': 264,\n",
       " 'will': 265,\n",
       " 'with': 266,\n",
       " 'you': 267,\n",
       " 'your': 268,\n",
       " 'yours': 269}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id # list of all tokens and their ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c452733",
   "metadata": {},
   "source": [
    "#### Create a gensim corpus:\n",
    "\n",
    "- uses a bag of words model\n",
    "- transforms each document into a bag of words using the token ids and their frequencies\n",
    "- gives a list of tuples (ID, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce3148d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 21),\n",
       "  (2, 5),\n",
       "  (3, 1),\n",
       "  (4, 7),\n",
       "  (5, 1),\n",
       "  (6, 2),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 2),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 2),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 2),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 2),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 2),\n",
       "  (54, 2),\n",
       "  (55, 1),\n",
       "  (56, 3),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 3),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 5),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 2),\n",
       "  (74, 2),\n",
       "  (75, 1),\n",
       "  (76, 2),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 2),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 1),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 2),\n",
       "  (94, 2),\n",
       "  (95, 1),\n",
       "  (96, 2),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 1),\n",
       "  (103, 1),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (106, 1),\n",
       "  (107, 1),\n",
       "  (108, 1),\n",
       "  (109, 3),\n",
       "  (110, 1),\n",
       "  (111, 1),\n",
       "  (112, 1),\n",
       "  (113, 1),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 1),\n",
       "  (123, 1),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 1),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 1),\n",
       "  (131, 1)],\n",
       " [(0, 2),\n",
       "  (1, 23),\n",
       "  (2, 6),\n",
       "  (3, 2),\n",
       "  (4, 8),\n",
       "  (10, 2),\n",
       "  (59, 2),\n",
       "  (132, 1),\n",
       "  (133, 2),\n",
       "  (134, 1),\n",
       "  (135, 3),\n",
       "  (136, 1),\n",
       "  (137, 3),\n",
       "  (138, 1),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (142, 1),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 1),\n",
       "  (147, 1),\n",
       "  (148, 1),\n",
       "  (149, 4),\n",
       "  (150, 2),\n",
       "  (151, 1),\n",
       "  (152, 4),\n",
       "  (153, 1),\n",
       "  (154, 1),\n",
       "  (155, 1),\n",
       "  (156, 1),\n",
       "  (157, 1),\n",
       "  (158, 1),\n",
       "  (159, 2),\n",
       "  (160, 1),\n",
       "  (161, 1),\n",
       "  (162, 2),\n",
       "  (163, 1),\n",
       "  (164, 1),\n",
       "  (165, 1),\n",
       "  (166, 1),\n",
       "  (167, 2),\n",
       "  (168, 1),\n",
       "  (169, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 2),\n",
       "  (173, 2),\n",
       "  (174, 1),\n",
       "  (175, 2),\n",
       "  (176, 1),\n",
       "  (177, 1),\n",
       "  (178, 1),\n",
       "  (179, 1),\n",
       "  (180, 1),\n",
       "  (181, 2),\n",
       "  (182, 1),\n",
       "  (183, 1),\n",
       "  (184, 1),\n",
       "  (185, 2),\n",
       "  (186, 1),\n",
       "  (187, 1),\n",
       "  (188, 1),\n",
       "  (189, 1),\n",
       "  (190, 1),\n",
       "  (191, 1),\n",
       "  (192, 1),\n",
       "  (193, 1),\n",
       "  (194, 1),\n",
       "  (195, 1),\n",
       "  (196, 1),\n",
       "  (197, 1),\n",
       "  (198, 1),\n",
       "  (199, 1),\n",
       "  (200, 1),\n",
       "  (201, 1),\n",
       "  (202, 2),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (205, 1),\n",
       "  (206, 1),\n",
       "  (207, 2),\n",
       "  (208, 2),\n",
       "  (209, 1),\n",
       "  (210, 1),\n",
       "  (211, 1),\n",
       "  (212, 1),\n",
       "  (213, 2),\n",
       "  (214, 1),\n",
       "  (215, 4),\n",
       "  (216, 11),\n",
       "  (217, 1),\n",
       "  (218, 1),\n",
       "  (219, 1),\n",
       "  (220, 1),\n",
       "  (221, 1),\n",
       "  (222, 1),\n",
       "  (223, 1),\n",
       "  (224, 3),\n",
       "  (225, 1),\n",
       "  (226, 2),\n",
       "  (227, 1),\n",
       "  (228, 1),\n",
       "  (229, 1),\n",
       "  (230, 1),\n",
       "  (231, 2),\n",
       "  (232, 1),\n",
       "  (233, 1),\n",
       "  (234, 1),\n",
       "  (235, 1),\n",
       "  (236, 1),\n",
       "  (237, 1),\n",
       "  (238, 3),\n",
       "  (239, 2),\n",
       "  (240, 1),\n",
       "  (241, 1),\n",
       "  (242, 1),\n",
       "  (243, 1),\n",
       "  (244, 1),\n",
       "  (245, 6),\n",
       "  (246, 18),\n",
       "  (247, 4),\n",
       "  (248, 2),\n",
       "  (249, 1),\n",
       "  (250, 2),\n",
       "  (251, 1),\n",
       "  (252, 3),\n",
       "  (253, 1),\n",
       "  (254, 1),\n",
       "  (255, 1),\n",
       "  (256, 4),\n",
       "  (257, 1),\n",
       "  (258, 2),\n",
       "  (259, 1),\n",
       "  (260, 1),\n",
       "  (261, 6),\n",
       "  (262, 1),\n",
       "  (263, 1),\n",
       "  (264, 1),\n",
       "  (265, 3),\n",
       "  (266, 1),\n",
       "  (267, 12),\n",
       "  (268, 3),\n",
       "  (269, 1)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an Mmcorpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9e5178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catilina\n",
      "[(0, 1), (1, 21), (2, 5), (3, 1), (4, 7), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Select the id for \"computer\": computer_id\n",
    "catilina_id = dictionary.token2id.get(\"Catilina\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(catilina_id))\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the first document\n",
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8338e9",
   "metadata": {},
   "source": [
    "- defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0\n",
    "- itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c4b09bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 21\n",
      "? 7\n",
      ". 5\n",
      "nihil 5\n",
      "iam 3\n"
     ]
    }
   ],
   "source": [
    "# Save the first document: doc\n",
    "doc = corpus[0]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f1c9190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 44\n",
      "the 18\n",
      "? 15\n",
      "you 12\n",
      ". 11\n"
     ]
    }
   ],
   "source": [
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238fb01",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f561f",
   "metadata": {},
   "source": [
    "- term frequency inverse document frequency\n",
    "- helps determine the most important word in each documents\n",
    "- stopwords are removed or underweighted\n",
    "- similarly, when going through areticles on the same topic, prevalent topic related words are downgraded\n",
    "- like 'sky' in astronomy (a common word for that topic)\n",
    "- weight calculated as number of occurences of the token in the given docment times log(total number of documents/number of documents that contains the token)\n",
    "- words that appear in most documents will have very low weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d29ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.06950480468569159),\n",
       " (6, 0.13900960937138318),\n",
       " (7, 0.06950480468569159),\n",
       " (8, 0.06950480468569159),\n",
       " (9, 0.06950480468569159),\n",
       " (11, 0.06950480468569159),\n",
       " (12, 0.06950480468569159),\n",
       " (13, 0.06950480468569159),\n",
       " (14, 0.06950480468569159),\n",
       " (15, 0.06950480468569159),\n",
       " (16, 0.06950480468569159),\n",
       " (17, 0.06950480468569159),\n",
       " (18, 0.06950480468569159),\n",
       " (19, 0.06950480468569159),\n",
       " (20, 0.06950480468569159),\n",
       " (21, 0.13900960937138318),\n",
       " (22, 0.06950480468569159),\n",
       " (23, 0.06950480468569159),\n",
       " (24, 0.06950480468569159),\n",
       " (25, 0.06950480468569159),\n",
       " (26, 0.06950480468569159),\n",
       " (27, 0.06950480468569159),\n",
       " (28, 0.06950480468569159),\n",
       " (29, 0.06950480468569159),\n",
       " (30, 0.06950480468569159),\n",
       " (31, 0.06950480468569159),\n",
       " (32, 0.13900960937138318),\n",
       " (33, 0.06950480468569159),\n",
       " (34, 0.06950480468569159),\n",
       " (35, 0.06950480468569159),\n",
       " (36, 0.06950480468569159),\n",
       " (37, 0.13900960937138318),\n",
       " (38, 0.06950480468569159),\n",
       " (39, 0.06950480468569159),\n",
       " (40, 0.06950480468569159),\n",
       " (41, 0.06950480468569159),\n",
       " (42, 0.06950480468569159),\n",
       " (43, 0.13900960937138318),\n",
       " (44, 0.06950480468569159),\n",
       " (45, 0.06950480468569159),\n",
       " (46, 0.06950480468569159),\n",
       " (47, 0.06950480468569159),\n",
       " (48, 0.06950480468569159),\n",
       " (49, 0.06950480468569159),\n",
       " (50, 0.06950480468569159),\n",
       " (51, 0.06950480468569159),\n",
       " (52, 0.06950480468569159),\n",
       " (53, 0.13900960937138318),\n",
       " (54, 0.13900960937138318),\n",
       " (55, 0.06950480468569159),\n",
       " (56, 0.20851441405707474),\n",
       " (57, 0.06950480468569159),\n",
       " (58, 0.06950480468569159),\n",
       " (60, 0.06950480468569159),\n",
       " (61, 0.06950480468569159),\n",
       " (62, 0.06950480468569159),\n",
       " (63, 0.06950480468569159),\n",
       " (64, 0.06950480468569159),\n",
       " (65, 0.06950480468569159),\n",
       " (66, 0.06950480468569159),\n",
       " (67, 0.06950480468569159),\n",
       " (68, 0.06950480468569159),\n",
       " (69, 0.06950480468569159),\n",
       " (70, 0.34752402342845795),\n",
       " (71, 0.06950480468569159),\n",
       " (72, 0.06950480468569159),\n",
       " (73, 0.13900960937138318),\n",
       " (74, 0.13900960937138318),\n",
       " (75, 0.06950480468569159),\n",
       " (76, 0.13900960937138318),\n",
       " (77, 0.06950480468569159),\n",
       " (78, 0.06950480468569159),\n",
       " (79, 0.06950480468569159),\n",
       " (80, 0.06950480468569159),\n",
       " (81, 0.13900960937138318),\n",
       " (82, 0.06950480468569159),\n",
       " (83, 0.06950480468569159),\n",
       " (84, 0.06950480468569159),\n",
       " (85, 0.06950480468569159),\n",
       " (86, 0.06950480468569159),\n",
       " (87, 0.06950480468569159),\n",
       " (88, 0.06950480468569159),\n",
       " (89, 0.06950480468569159),\n",
       " (90, 0.06950480468569159),\n",
       " (91, 0.06950480468569159),\n",
       " (92, 0.06950480468569159),\n",
       " (93, 0.13900960937138318),\n",
       " (94, 0.13900960937138318),\n",
       " (95, 0.06950480468569159),\n",
       " (96, 0.13900960937138318),\n",
       " (97, 0.06950480468569159),\n",
       " (98, 0.06950480468569159),\n",
       " (99, 0.06950480468569159),\n",
       " (100, 0.06950480468569159),\n",
       " (101, 0.06950480468569159),\n",
       " (102, 0.06950480468569159),\n",
       " (103, 0.06950480468569159),\n",
       " (104, 0.06950480468569159),\n",
       " (105, 0.06950480468569159),\n",
       " (106, 0.06950480468569159),\n",
       " (107, 0.06950480468569159),\n",
       " (108, 0.06950480468569159),\n",
       " (109, 0.20851441405707474),\n",
       " (110, 0.06950480468569159),\n",
       " (111, 0.06950480468569159),\n",
       " (112, 0.06950480468569159),\n",
       " (113, 0.06950480468569159),\n",
       " (114, 0.06950480468569159),\n",
       " (115, 0.06950480468569159),\n",
       " (116, 0.06950480468569159),\n",
       " (117, 0.06950480468569159),\n",
       " (118, 0.06950480468569159),\n",
       " (119, 0.06950480468569159),\n",
       " (120, 0.06950480468569159),\n",
       " (121, 0.06950480468569159),\n",
       " (122, 0.06950480468569159),\n",
       " (123, 0.06950480468569159),\n",
       " (124, 0.06950480468569159),\n",
       " (125, 0.06950480468569159),\n",
       " (126, 0.06950480468569159),\n",
       " (127, 0.06950480468569159),\n",
       " (128, 0.06950480468569159),\n",
       " (129, 0.06950480468569159),\n",
       " (130, 0.06950480468569159),\n",
       " (131, 0.06950480468569159)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c86ddb",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "- use the StanfordNLP library integrated in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e80453cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Komrom', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('city', 'NN'), ('in', 'IN')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Komrom is a city in Hungary on the south bank of the Danube in Komrom-Esztergom county. \n",
    "Komrno, Slovakia, is on the northern bank. Komrom was formerly a separate village called jszny. \n",
    "In 1892 Komrom and jszny were connected with an iron bridge and in 1896 the two towns were united \n",
    "under the name city of Komrom. The fortress played an important role in the Hungarian Revolution of 1848 \n",
    "and many contemporary English sources refer to it as the Fortress of Comorn\"\"\"\n",
    "\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent) # tag for parts of speach\n",
    "tagged_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b320e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Komrom/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  city/NN\n",
      "  in/IN\n",
      "  (GPE Hungary/NNP)\n",
      "  on/IN\n",
      "  the/DT\n",
      "  south/JJ\n",
      "  bank/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE Danube/NNP)\n",
      "  in/IN\n",
      "  Komrom-Esztergom/NNP\n",
      "  county/NN\n",
      "  ./.\n",
      "  (PERSON Komrno/NNP)\n",
      "  ,/,\n",
      "  (GPE Slovakia/NNP)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  on/IN\n",
      "  the/DT\n",
      "  northern/JJ\n",
      "  bank/NN\n",
      "  ./.\n",
      "  (PERSON Komrom/NNP)\n",
      "  was/VBD\n",
      "  formerly/RB\n",
      "  a/DT\n",
      "  separate/JJ\n",
      "  village/NN\n",
      "  called/VBN\n",
      "  (PERSON jszny/NNP)\n",
      "  ./.\n",
      "  In/IN\n",
      "  1892/CD\n",
      "  Komrom/NNP\n",
      "  and/CC\n",
      "  (PERSON jszny/NNP)\n",
      "  were/VBD\n",
      "  connected/VBN\n",
      "  with/IN\n",
      "  an/DT\n",
      "  iron/NN\n",
      "  bridge/NN\n",
      "  and/CC\n",
      "  in/IN\n",
      "  1896/CD\n",
      "  the/DT\n",
      "  two/CD\n",
      "  towns/NNS\n",
      "  were/VBD\n",
      "  united/VBN\n",
      "  under/IN\n",
      "  the/DT\n",
      "  name/NN\n",
      "  city/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION Komrom/NNP)\n",
      "  ./.\n",
      "  The/DT\n",
      "  fortress/NN\n",
      "  played/VBD\n",
      "  an/DT\n",
      "  important/JJ\n",
      "  role/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (GPE Hungarian/JJ)\n",
      "  Revolution/NNP\n",
      "  of/IN\n",
      "  1848/CD\n",
      "  and/CC\n",
      "  many/JJ\n",
      "  contemporary/JJ\n",
      "  English/JJ\n",
      "  sources/NNS\n",
      "  refer/VBP\n",
      "  to/TO\n",
      "  it/PRP\n",
      "  as/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Fortress/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION Comorn/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863687a",
   "metadata": {},
   "source": [
    "Gives a tree with leaves, subtrees etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d622c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"Komrom is a city in Hungary on the south bank of the Danube in Komrom-Esztergom county. \n",
    "Komrno, Slovakia, is on the northern bank. Komrom was formerly a separate village called jszny. \n",
    "In 1892 Komrom and jszny were connected with an iron bridge and in 1896 the two towns were united \n",
    "under the name city of Komrom. The fortress played an important role in the Hungarian Revolution of 1848 \n",
    "and many contemporary English sources refer to it as the Fortress of Comorn\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb3d272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary = True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NNP\": # where is the error here?\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ce72e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Komrom', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('city', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('Hungary', 'NNP'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('south', 'JJ'),\n",
       "  ('bank', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Danube', 'NNP'),\n",
       "  ('in', 'IN'),\n",
       "  ('Komrom-Esztergom', 'NNP'),\n",
       "  ('county', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Komrno', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('Slovakia', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('is', 'VBZ'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('northern', 'JJ'),\n",
       "  ('bank', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Komrom', 'NNP'),\n",
       "  ('was', 'VBD'),\n",
       "  ('formerly', 'RB'),\n",
       "  ('a', 'DT'),\n",
       "  ('separate', 'JJ'),\n",
       "  ('village', 'NN'),\n",
       "  ('called', 'VBN'),\n",
       "  ('jszny', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('In', 'IN'),\n",
       "  ('1892', 'CD'),\n",
       "  ('Komrom', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('jszny', 'NNP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('connected', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('an', 'DT'),\n",
       "  ('iron', 'NN'),\n",
       "  ('bridge', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('in', 'IN'),\n",
       "  ('1896', 'CD'),\n",
       "  ('the', 'DT'),\n",
       "  ('two', 'CD'),\n",
       "  ('towns', 'NNS'),\n",
       "  ('were', 'VBD'),\n",
       "  ('united', 'VBN'),\n",
       "  ('under', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('name', 'NN'),\n",
       "  ('city', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Komrom', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('fortress', 'NN'),\n",
       "  ('played', 'VBD'),\n",
       "  ('an', 'DT'),\n",
       "  ('important', 'JJ'),\n",
       "  ('role', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Hungarian', 'JJ'),\n",
       "  ('Revolution', 'NNP'),\n",
       "  ('of', 'IN'),\n",
       "  ('1848', 'CD'),\n",
       "  ('and', 'CC'),\n",
       "  ('many', 'JJ'),\n",
       "  ('contemporary', 'JJ'),\n",
       "  ('English', 'JJ'),\n",
       "  ('sources', 'NNS'),\n",
       "  ('refer', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('it', 'PRP'),\n",
       "  ('as', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Fortress', 'NNP'),\n",
       "  ('of', 'IN'),\n",
       "  ('Comorn', 'NNP')]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4760882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bfdbc63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels # again this gives nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "031bc9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAACtklEQVR4nO3TMQEAIAzAMMC/52GAnx6Jgj7dM7OAnvM7AHgzJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNC1AVcegTL+uSnUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140, normalize = False)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5a2ce",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "661b5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2cafcea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a big dog\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # load the English model\n",
    "doc = nlp(\"There is a big dog.\")    # process a text and create a Doc object\n",
    "for chunk in doc.noun_chunks:       # iterate over the noun chunks in the Doc\n",
    "   print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8740e131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Hungary,\n",
       " Komrno,\n",
       " Slovakia,\n",
       " jszny,\n",
       " 1892,\n",
       " jszny,\n",
       " 1896,\n",
       " two,\n",
       " Komrom,\n",
       " the Hungarian Revolution,\n",
       " 1848,\n",
       " English,\n",
       " the Fortress of Comorn)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = nlp(\"\"\"Komrom is a city in Hungary on the south bank of the Danube in Komrom-Esztergom county. \n",
    "Komrno, Slovakia, is on the northern bank. Komrom was formerly a separate village called jszny. \n",
    "In 1892 Komrom and jszny were connected with an iron bridge and in 1896 the two towns were united \n",
    "under the name city of Komrom. The fortress played an important role in the Hungarian Revolution of 1848 \n",
    "and many contemporary English sources refer to it as the Fortress of Comorn\"\"\")\n",
    "docs.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0b09e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hungary GPE\n"
     ]
    }
   ],
   "source": [
    "print(docs.ents[0], docs.ents[0].label_) # gives the label (GPE = geopolitical entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64f46b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE Hungary\n",
      "PERSON Komrno\n",
      "GPE Slovakia\n",
      "ORG jszny\n",
      "DATE 1892\n",
      "PERSON jszny\n",
      "DATE 1896\n",
      "CARDINAL two\n",
      "GPE Komrom\n",
      "EVENT the Hungarian Revolution\n",
      "DATE 1848\n",
      "LANGUAGE English\n",
      "ORG the Fortress of Comorn\n"
     ]
    }
   ],
   "source": [
    "# Print all of the found entities and their labels\n",
    "for ent in docs.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee7e8b",
   "metadata": {},
   "source": [
    "- spacy has easy pipeline creation\n",
    "- and different entity labels from nltk\n",
    "- informal language formula allows easier work with tweets and chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d17975",
   "metadata": {},
   "source": [
    "### Machine Learning and Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45739435",
   "metadata": {},
   "source": [
    "#### Using CountVectorizer (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6638238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn. model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "671061d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillarys Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3   Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
      " '000km']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../fake_or_real_news.csv\")\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train) # when we use y_train?\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c30116",
   "metadata": {},
   "source": [
    "#### Using TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "faffbbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
      " '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = \"english\", max_df = 0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5]) # A. is the array attribute of the train object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f38624e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "                     ade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "                         ade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns = tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0a2f6",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98d93d",
   "metadata": {},
   "source": [
    "- each word from CountVectorizer acts as a feature\n",
    "- Naive Bayes expects integers as inputs\n",
    "- use SVM with floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e7b9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5b329fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f08ed54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.893352462936394"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "779d042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c243c",
   "metadata": {},
   "source": [
    "#### Naive Bayes using Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "005e1b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404694c8",
   "metadata": {},
   "source": [
    "#### Optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f94b3934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  1e-05\n",
      "Score:  0.8981348637015782\n",
      "\n",
      "Alpha:  0.09999999999999999\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.19999\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.29997999999999997\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.39997\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.49996\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.5999499999999999\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.6999399999999999\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.7999299999999999\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.8999199999999999\n",
      "Score:  0.8589191774270684\n",
      "\n",
      "Alpha:  0.99991\n",
      "Score:  0.8565279770444764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0.00001, 1, 0.09999)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha = alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f9afcf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
      "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0],feature_names))\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970f3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
